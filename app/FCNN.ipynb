{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch FCNN Churn Prediction\n",
    "We want to build and save a model, ready for deployment, using PyTorch. The model should be capable of taking in data about an arbitrary customer and predict wether or not said customer is likely to churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n",
    "The dataset used with this model is publicly available on [kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn/data) and consists of both text and numerical values. To this end, a rigorous data preprocessing pipeline is necessary to facilitate supervised learning and later predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# Load in the data from the local repository\n",
    "#   NOTE: We do so with BASE_DIR to ensure localization on different machines\n",
    "BASE_DIR = Path(\"FCNN.ipynb\").parent.resolve()\n",
    "data = pd.read_csv(f\"{BASE_DIR}/data/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Pipeline (NEEDS UPDATE)\n",
    "The dataset contains a fair share of problematic instances. To summarize, we need to handle the following:\n",
    "- **Remove redundant columns:** Certain data is irrelevant when predicting customer churn. In our case, the most prevalent one is `customerID`, which should be disregarded.\n",
    "- **Missing values:** A few instances see missing data in one or more columns. There are a few ways to handle such instances, like interpolating data or simply filling with a mean value (using f.ex `sklearn.impute.SimpleImputer`), however, given the size of the dataset and the potential of adding erroneous information, the adopted strategy is to simply remove incomplete rows of data using `data.dropna(inplace = True)`.\n",
    "- **Text values:** The dataset contains a mix of datatypes, namely numerical ones and text, the latter of which is not well-suited for ML training or prediciton.\n",
    "    - **Binary text data:** For text data that is binary in nature, we will simply encode the binary cases to 1's and 0's (f.ex in `gender`, consider _male_ -> 0 and _female_ -> 1).\n",
    "    - **Non-Binary text data:** For text data that is not binary in nature, we will use a _One Hot Encoder_ from `sklearn.preprocessing.OneHotEncoder`. This is to be preferred over assigning numerical values, as we remove some spatial bias from the final classifier.\n",
    "\n",
    "For convenience and simplicity of use, we will define a set of transformer classes (inheriting from `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin`) which are integrated into an sklearn Pipeline `sklearn.pipeline.Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.Preprocessing import ColumnDropper, NaNAmputator, FeatureEncoder, DataValidator\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Put together the final Pipeline\n",
    "preprocessing = Pipeline([\n",
    "    (\"dropper\", ColumnDropper()),\n",
    "    (\"amputator\", NaNAmputator()),\n",
    "    (\"feature_encoder\", FeatureEncoder()),\n",
    "    (\"data_validator\", DataValidator())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "data = preprocessing.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to PyTorch tensors\n",
    "An important step in any PyTorch application is to convert datasets to the correct datatype. In this case, we want to convert our pandas dataframe into pytorch tensors with datatype `torch.float32`.\n",
    "\n",
    "Note that to do this, we need to extract the labels from the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up some device agnostic code\n",
    "#   NOTE: If one has an NVIDIA GPU, device can be set to 'cuda' for increased performance.\n",
    "#         As my current dev. rig has an AMD GPU, the device is set to 'cpu'\n",
    "device = \"cpu\"\n",
    "\n",
    "# Extract the truth-labels from the data\n",
    "labels = data[['Churn']]\n",
    "data.drop(columns = ['Churn'], inplace = True)\n",
    "\n",
    "# Convert the data into a tensor\n",
    "data = torch.from_numpy(data.values).type(torch.float32).to(device)\n",
    "labels = torch.from_numpy(labels.values).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the binary classifier\n",
    "We seek to build and train a Fully Connected Neural Network (FCNN) using PyTorch, and train it on a subset of the churn data. Note that the code for the FCNN can be found in `models.FCNN.py` and we only construct the imported class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_models.models.FCNN import FCNN\n",
    "\n",
    "# Define an instance of the model and place it on the correct device\n",
    "model = FCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the FCNN\n",
    "To train the network, we need 3 things: \n",
    "1. To split the available data into a training and test set. This is done by the popular method `sklearn.model_selection.train_test_split()`.\n",
    "2. Declare an optimizer and a loss function. The popular `torch.optim.Adam()` optimizer is adopted, as well as the loss function `torch.nn.BCEWithLogitsLoss()`, which is ideal for a binary classification problem. \n",
    "3. Declare a training and test loop to train and evaluate the model's performance. The training loop normally consists of 5 repeatable steps and the test loop is mainly to asses improvements over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the desired loss function and optimizer\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(),\n",
    "                            lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 6.201237678527832 | Test loss: 20.52345085144043\n",
      "Epoch: 20 | Loss: 1.9906054735183716 | Test loss: 2.7850170135498047\n",
      "Epoch: 40 | Loss: 0.9085407853126526 | Test loss: 1.4036239385604858\n",
      "Epoch: 60 | Loss: 0.46694180369377136 | Test loss: 0.44691285490989685\n",
      "Epoch: 80 | Loss: 0.7888277769088745 | Test loss: 0.5469276309013367\n",
      "Epoch: 100 | Loss: 0.5213155746459961 | Test loss: 2.112501859664917\n",
      "Epoch: 120 | Loss: 0.47783735394477844 | Test loss: 0.5143683552742004\n",
      "Epoch: 140 | Loss: 1.21821129322052 | Test loss: 0.8596041202545166\n",
      "Epoch: 160 | Loss: 0.5251643061637878 | Test loss: 0.5622532367706299\n",
      "Epoch: 180 | Loss: 0.5491958260536194 | Test loss: 0.6347532868385315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make sure all data is present on the right device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Define a training/test loop\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Set the model into training mode -> Turn on gradient tracking\n",
    "    model.train()\n",
    "\n",
    "    # 1. Perform a forward pass of the training data\n",
    "    # NOTE: When performing the normal pass, we end up with logits, necessary for loss_fn in step 2\n",
    "    y_train_logits = model(X_train)\n",
    "    y_train_preds = torch.round(torch.sigmoid(y_train_logits))\n",
    "\n",
    "    # 2. Calculate the loss (using logits)\n",
    "    loss = loss_fn(y_train_logits, y_train)\n",
    "    #acc = accuracy_score(y_train, y_train_preds)\n",
    "\n",
    "    # 3. Reset the gradients of the optimizer (Otherwise they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Calculate the gradient shifts to all parameters with a backwards pass\n",
    "    loss.backward() # Backpropagation\n",
    "\n",
    "    # 5. Update model parameters with a step in the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # TEST LOOP\n",
    "    # Set the model to evaluation mode -> Turn off gradient tracking\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Perform a forward pass of the test data\n",
    "        y_test_logits = model(X_test)\n",
    "        y_test_preds = torch.round(torch.sigmoid(y_test_logits))\n",
    "\n",
    "        # Calculate the loss and other performance metrics\n",
    "        test_loss = loss_fn(y_test_logits, y_test)\n",
    "        #test_acc = accuracy_score(y_test, y_test_preds)\n",
    "\n",
    "    # Print out some performance metrics to track the improvement during training\n",
    "    if epoch%(epochs//10) == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model & Preprocessing pipeline\n",
    "Finally, we want the model to be saved for quicker deployment as part of an API, as well as the adopted preprocessing pipeline. To this end, the `pathlib` module and `torch.save()` will be used to save the model, whilst the pipeline will be serialized using the `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle\n",
    "\n",
    "# Define the path in which to save the model\n",
    "MODEL_PATH = Path(f\"{BASE_DIR}/ml_models/trained_models\")\n",
    "MODEL_PATH.mkdir(parents = True, exist_ok = True)   # If no 'models' folder -> mkdir\n",
    "\n",
    "# Create a direct path to save the model as a .pth file\n",
    "MODEL_NAME = 'FCNN_churn_V0.pth'\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# Save the models parameters (state_dict)\n",
    "torch.save(model, MODEL_SAVE_PATH)\n",
    "\n",
    "# Define a path in which to save the pipeline\n",
    "PIPELINE_PATH = Path(f\"{BASE_DIR}/pipelines/completed_pipelines\")\n",
    "PIPELINE_PATH.mkdir(parents = True, exist_ok = True)    # if no 'pipelines' folder -> mkdir\n",
    "\n",
    "# Create a direct path to save the pipeline object as a .pkl file\n",
    "PIPELINE_NAME = 'churn_preprocessing_V0.pkl'\n",
    "PIPELINE_SAVE_PATH = PIPELINE_PATH / PIPELINE_NAME\n",
    "\n",
    "# Save the model by dumping the object to a pickle file\n",
    "with open(PIPELINE_SAVE_PATH, \"wb\") as f:\n",
    "    _pickle.dump(preprocessing, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
